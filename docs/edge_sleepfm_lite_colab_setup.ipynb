{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge SleepFM-Lite: Colab Setup\\n",
    "\\n",
    "Thi\u1ebft l\u1eadp m\u00f4i tr\u01b0\u1eddng v\u00e0 bi\u1ebfn d\u00f9ng chung cho pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q numpy pandas pyarrow h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_DIR = '/content/drive/MyDrive/edge-sleepfm-lite'\n",
    "RAW_DIR = os.path.join(PROJECT_DIR, 'data', 'raw')\n",
    "HDF5_DIR = os.path.join(PROJECT_DIR, 'data', 'canonical_hdf5')\n",
    "PARQUET_DIR = os.path.join(PROJECT_DIR, 'data', 'training_store')\n",
    "SPLIT_PATH = os.path.join(PROJECT_DIR, 'data', 'splits')\n",
    "TEACHER_TARGET_DIR = os.path.join(PROJECT_DIR, 'data', 'teacher_targets')\n",
    "CHECKPOINT_DIR = os.path.join(PROJECT_DIR, 'checkpoints')\n",
    "\n",
    "print('PROJECT_DIR:', PROJECT_DIR)\n",
    "print('RAW_DIR:', RAW_DIR)\n",
    "print('HDF5_DIR:', HDF5_DIR)\n",
    "print('PARQUET_DIR:', PARQUET_DIR)\n",
    "print('SPLIT_PATH:', SPLIT_PATH)\n",
    "print('TEACHER_TARGET_DIR:', TEACHER_TARGET_DIR)\n",
    "print('CHECKPOINT_DIR:', CHECKPOINT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sha2, substring, countDistinct, max as spark_max, when\n",
    "\n",
    "spark = SparkSession.builder.appName(\"sleepfm-index-split\").getOrCreate()\n",
    "\n",
    "metadata_path = os.path.join(RAW_DIR, 'metadata.csv')\n",
    "meta_df = spark.read.csv(metadata_path, header=True)\n",
    "\n",
    "meta_df = meta_df.withColumn('subject_hash', sha2(col('subject_id'), 256))\n",
    "\n",
    "hash_prefix = substring(col('subject_hash'), 1, 2)\n",
    "split_expr = (\n",
    "    when(hash_prefix <= 'aa', 'train')\n",
    "    .when(hash_prefix <= 'cc', 'val')\n",
    "    .otherwise('test')\n",
    ")\n",
    "meta_df = meta_df.withColumn('split', split_expr)\n",
    "\n",
    "train_df = meta_df.filter(col('split') == 'train')\n",
    "val_df = meta_df.filter(col('split') == 'val')\n",
    "test_df = meta_df.filter(col('split') == 'test')\n",
    "\n",
    "train_df.write.mode('overwrite').parquet(os.path.join(SPLIT_PATH, 'train'))\n",
    "val_df.write.mode('overwrite').parquet(os.path.join(SPLIT_PATH, 'val'))\n",
    "test_df.write.mode('overwrite').parquet(os.path.join(SPLIT_PATH, 'test'))\n",
    "\n",
    "split_counts = (meta_df\n",
    "    .groupBy('subject_id')\n",
    "    .agg(countDistinct('split').alias('split_count'))\n",
    ")\n",
    "max_splits = split_counts.agg(spark_max('split_count').alias('max_split')).collect()[0]['max_split']\n",
    "assert max_splits == 1, f'Subjects found in multiple splits: max split count = {max_splits}'\n",
    "print('Verified: each subject_id is assigned to a single split.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}